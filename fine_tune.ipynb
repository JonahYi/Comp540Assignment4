{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpUSzpBvFokm"
      },
      "source": [
        "# Model Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWwzph_dFokp"
      },
      "source": [
        "**OPTIONAL**: Follow the instructions below to fine-tune your model!\n",
        "\n",
        "If you would like to do a fine-tune run that is longer than Google CoLab will allow, **email me (amu1@rice.edu)** your notebook and I will run it for you on the VM and send you the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXmeiQP4Fokq"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgQ4ltEpFokr"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedModel, PreTrainedTokenizer, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, TaskType\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-0bOIGIFoks"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYw14I5JFoks",
        "outputId": "d2f12f58-a878-4bbd-f9c6-22a5d5794f7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/sdb/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from getpass import getpass\n",
        "import os\n",
        "import torch\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
        "HF_CACHE = os.environ.get(\"HF_CACHE\", None)\n",
        "if not HF_TOKEN:\n",
        "  HF_TOKEN = getpass(\"Enter your HuggingFace token:\")\n",
        "\n",
        "HF_MODEL_NAME=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Here we compress our model with 8-bit \"quantization\".\n",
        "# You can think of quantization as rounding the model's\n",
        "# parameters, which are stored as 32-bit floats, to 8 bits.\n",
        "# This saves a lot of space on our GPU!\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "  load_in_8bit=True,\n",
        "  bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  HF_MODEL_NAME,\n",
        "  cache_dir=HF_CACHE\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Notice the 'Causal' ain the constructor.\n",
        "# Llama is a 'Causal' LM, meaning it predicts the next token given only the previous tokens.\n",
        "# Some 'masked' LMs like BERT can predict a token in the middle of a sentence.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  HF_MODEL_NAME,\n",
        "  quantization_config=quantization_config,\n",
        "  # Let HuggingFace decide which device to put our model on.\n",
        "  # This will efficiently share CPU and GPU resources.\n",
        "  device_map=\"auto\",\n",
        "  token=HF_TOKEN\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCZ0vYMOFokt"
      },
      "source": [
        "# Set up LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z4ShYj2Fokt"
      },
      "source": [
        "LoRA (Low-Rank Analysis) is a great approach to fine-tuning in resource-constrained spaces.\n",
        "\n",
        "Here is a short summary with (a LOT) of details missing\n",
        "\n",
        "1) The original model's weights are fixed, meaning they remain unchanged during the training process.\n",
        "2) For a selected subset of layers (generally attention layers), a new matrix is created. The weights of this matrix are trainable\n",
        "3) During the model's forward function, for each layer, encodings are multiplied through the original model matrix and the LoRA matrix.\n",
        "4) The output of the two matrix multiplications are summed before being passed on to the next layer.\n",
        "\n",
        "A much better explanation can be found [here](https://codecompass00.substack.com/p/what-is-lora-a-visual-guide-llm-fine-tuning) for those who are curious.\n",
        "\n",
        "You will be shocked at how well a LoRA fine-tuned model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCpmfpP6Foku"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "  task_type=TaskType.CAUSAL_LM,\n",
        "  # The rank of the LoRA matrix added alongside the original layer.\n",
        "  # Higher rank --> more tunable parameters\n",
        "  # Recommended to use powers of 2.\n",
        "  r=8,\n",
        "\n",
        "  # Names of modules (layers) that LoRA will target.\n",
        "  # You can print a model's layers with model.modules().\n",
        "  # Generally, self-attention layers\n",
        "  # (like these) are targeted because they have little effect on the model's\n",
        "  # encoding process, but a large effect on what parts of the sequence the model\n",
        "  # pays attention to. So, you can change your output drastically without\n",
        "  # touching the model's underlying knowledge.\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "\n",
        "  # Before the LoRA matrix encoding is added to the original model's encoding,\n",
        "  # the encoding is multiplied by this constant factor.\n",
        "  # So, higher alpha --> LoRA layers have more effect on model.\n",
        "  # Powers of 2 are most commonly used here.\n",
        "  lora_alpha=16,\n",
        "\n",
        "  # Higher dropout induces random noise and sets random parameters to 0 during training.\n",
        "  # Keep this small! (< 0.2)\n",
        "  lora_dropout=0.1,\n",
        ")\n",
        "model_for_training = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1uRvDU3Foku",
        "outputId": "233ac9ee-c5d5-4275-c51d-95e6fca069ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,780,672 || all params: 3,223,530,496 || trainable%: 0.3344\n"
          ]
        }
      ],
      "source": [
        "model_for_training.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6llGpzsDFoku"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mItT4JYgFoku"
      },
      "source": [
        "Ideally, your dataset should consist of prompts in the Llama Instruct format, like:\n",
        "\n",
        "> <|begin_of_text|>\n",
        ">\n",
        "> <|start_header_id|>system<|end_header_id|>\n",
        ">\n",
        "> {Instruction you want your model to respond to}\n",
        ">\n",
        "> <|eot_id|>\n",
        ">\n",
        "> <|start_header_id|>user<|end_header_id|>\n",
        ">\n",
        "> {Example input}\n",
        ">\n",
        "> <|eot_id|>\n",
        ">\n",
        "> <|start_header_id|>assistant<|end_header_id|>\n",
        ">\n",
        "> {Desired output}\n",
        ">\n",
        "> <|eos_token|>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6fa9jMAFoku"
      },
      "source": [
        "An easy place to start fine-tuning is to pick a few outputs from your base model that you wish to improve, edit them, and then train your model on the edited version.\n",
        "\n",
        "The provided code will work if you have done the following:\n",
        "1) Saved **youtube_df** or **reddit_df**, or a random sample of them (with df.sample(n_samples)), to CSV\n",
        "    - About 100 samples will do as a starting point\n",
        "2) Edited the **inference_output {TOPIC}** columns to your desired output\n",
        "3) Optionally, edited the **full_prompt {TOPIC}** columns if you would like the model to respond to a different (e.g. a shorter) prompt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXlK-MusFokv"
      },
      "outputs": [],
      "source": [
        "# Load your spreadsheet\n",
        "import re\n",
        "\n",
        "EDITED_CSV_PATH = \"reddit.csv\"\n",
        "edited_df = pd.read_csv(EDITED_CSV_PATH)\n",
        "\n",
        "topics = [\n",
        "    column.removeprefix(\"inference_output \")\n",
        "    for column in edited_df.columns\n",
        "    if re.match(\"inference_output .*\", column)\n",
        "]\n",
        "\n",
        "full_text = []\n",
        "for topic in topics:\n",
        "    full_text.extend(\n",
        "        edited_df[f\"full_prompt {topic}\"] + edited_df[f\"inference_output {topic}\"] + \"<|eot_id|>\"\n",
        "    )\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    \"text\": full_text\n",
        "})\n",
        "\n",
        "train_test_split = dataset.train_test_split(0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_M4juM3Fokv",
        "outputId": "81e62bfc-aa6c-4d99-a485-3823551fc1d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/sdb/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/mnt/sdb/miniconda3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/mnt/sdb/miniconda3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 16/16 [00:00<00:00, 1175.41 examples/s]\n",
            "Map: 100%|██████████| 4/4 [00:00<00:00, 714.53 examples/s]\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    # Absolute path to store intermediate files\n",
        "    # during training. Feel free to change this.\n",
        "    output_dir=\"lora_layers/train\",\n",
        "\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-3,\n",
        "\n",
        "    # Number of samples on each core per batch.\n",
        "    # Raising this makes training faster but takes up more GPU space.\n",
        "    per_device_train_batch_size=2,\n",
        "\n",
        "    # Number of epochs\n",
        "    # Advised that you raise this after you do 1 run to test\n",
        "    num_train_epochs=2,\n",
        "\n",
        "    # When to run over the validation set.\n",
        "    # Set this to 'step' if you would like evaluation on every batch.\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # Use 16-bit floats instead of 32-bit (saves vRAM)\n",
        "    fp16=True, # Consider this!\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model_for_training,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    # Transformers dataset\n",
        "    train_dataset=train_test_split[\"train\"],\n",
        "    eval_dataset=train_test_split[\"test\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        # causal model, not masked language model.\n",
        "        mlm=False\n",
        "    ),\n",
        "    dataset_text_field=\"text\",\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=512\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJRGDwvlFokv"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iek7BmXlFokv",
        "outputId": "2c013f4e-79dc-44af-c490-d1982a75f0b2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:22, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.566039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.505438</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Disables KV caching, which stores tensors computed during\n",
        "# passes over the attention layer instead of recomputing them\n",
        "# for each token.\n",
        "# Much slower, but large savings in GPU memory.\n",
        "# Crucial during training!\n",
        "model.config.use_cache = False\n",
        "model_for_training.config.use_cache = False\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.config.use_cache = True\n",
        "model_for_training.config.use_cache = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP2KHfb8Fokv"
      },
      "source": [
        "# Save output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3OKJcpFokw"
      },
      "source": [
        "All done! Now, save your layer somewhere. Uncomment the code in the **Load Model** section of analyze_social_media.ipynb to load in your layer!\n",
        "\n",
        "You can also port over the inference() function from there to test your model now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoBYuL49Fokw"
      },
      "outputs": [],
      "source": [
        "# Feel free to change this.\n",
        "model_for_training.save_pretrained(\"lora\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
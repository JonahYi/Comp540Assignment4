{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zksu71N8b3JB"
      },
      "source": [
        "# Social Media Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3XBBrsICrn8"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaIf1oplb3JC"
      },
      "source": [
        "In this election year in the United States, your team will make use of an LLM to analyze what people on social media think of the two presidential candidates (Harris and Trump) on a range of election-relevant issues (e.g., immigration, the economy, the war in Gaza, the war in Ukraine, Medicare). You will do this by reading comments from Reddit and YouTube and asking the LLM how the commenter feels about your candidate and topics of interest. We will run the provided code (with your modifications) code each day to get a picture of the public's opinion on your candidate and issues and how it changes all the way to November 5, election day!\n",
        "\n",
        "This problem, called [Feature-Based or Aspect-Based Sentiment Analysis](https://paperswithcode.com/task/aspect-based-sentiment-analysis#task-home), is a classic one in NLP, and has many use cases. Marketing firms, for exmaple, will use it to identify which aspects of their product people like and dislike."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rePp5ACC11b"
      },
      "source": [
        "### Your Responsibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLs_DJ3xb3JD"
      },
      "source": [
        "At minimum, you will be responsible for:\n",
        "1. Modifying our data collection method (see \"Data Collection\" section)\n",
        "2.  Writing prompts based on ours for a few topics of your choosing (see the \"System prompts\" section, and read through the previous sections for an explanation).\n",
        "3.  Providing a brief justification of your methodology for both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF9D55LSCvuF"
      },
      "source": [
        "### Developing your Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvlHXAW8b3JD"
      },
      "source": [
        "The code in this notebook makes use of a GPU, and needs at least 16GB vRAM to work. Fortunately, Rice has provided us with two ways to run your code - a VM with the needed resources and access to NOTS. Also, [Google CoLab](https://colab.research.google.com) offers a T4 GPU with 16GB for free, although there is a limit per user per day.\n",
        "\n",
        "When developing and testnig your prompts, run your code using CoLab. Make sure you select the T4 runtime. You can do this by clicking the arrow next to \"Connect\" in the top right, selecting \"Change Runtime Type\", then \"T4 GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8XTHDRVFjHF"
      },
      "source": [
        "### Doing a full dataset run of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydKVc5mGDHBU"
      },
      "source": [
        "\n",
        "Google CoLab's free tier will disconnect you after a few hours, so it is best for short inference runs. This is good for testing your prompts, but will not do for running inference on the full dataset that you collect.\n",
        "\n",
        "Starting on Friday, October 25th, and every 2 days thereafter, an assignment will be due on Gradescope called **Special LLM Assignment: Checkpoint \"n\"**. Submit your most recent version of this notebook to that assignment, and we will do a full dataset run of your code! Ensure that **all cells will run sequentially** without errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J88UAexGwnu"
      },
      "source": [
        "## API Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsMespGpGeqe"
      },
      "source": [
        "\n",
        "You will need access to various APIs in order to run the code.\n",
        "\n",
        "The [Guide to API Access](https://docs.google.com/document/d/1oPmxtLhJEH_F1E-WXJtG1aXlyB5tZCestcXKdShSZdQ/edit?usp=sharing) will walk you through how to gain acces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z75ny6_mb3JD"
      },
      "source": [
        "## Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-OcaHGQb3JD"
      },
      "source": [
        "Run this code only if running this notebook on Google CoLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4nf9EsBeb3JD"
      },
      "outputs": [],
      "source": [
        "%pip install praw\n",
        "%pip install peft\n",
        "%pip install trl\n",
        "%pip install bitsandbytes\n",
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-gRctgab3JE"
      },
      "source": [
        "Run this code regardless of your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dBtIBayb3JE"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import googleapiclient.discovery\n",
        "import praw\n",
        "import praw.models\n",
        "from getpass import getpass\n",
        "import itertools\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedModel, PreTrainedTokenizer, AutoConfig\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from getpass import getpass\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqy2b6xCb3JE"
      },
      "source": [
        "## Data Collection (MODIFY THIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-DgHHedb3JE"
      },
      "source": [
        "The code in this section collects and formats all the data we will use from YouTube and Reddit.\n",
        "\n",
        "The first 3 subsections (Connect to API Clients, Fetch YouTube Data, and Fetch Reddit Data) contain a host of utility functions that you will make use of to collect the data.\n",
        "\n",
        "You will be responsible for modifying the \"Put it all together and collect the data\" section.\n",
        "\n",
        "Follow the **Guide to API Access** in order to retrieve the API credentials that this function will prompt you for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THI-C1-Ab3JE"
      },
      "source": [
        "### Connect to API Clients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBuFY249b3JE"
      },
      "source": [
        "The code in these cells opens up a connection to the YouTube and Reddit APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozibxRBvb3JE"
      },
      "outputs": [],
      "source": [
        "def connect_to_youtube():\n",
        "    \"\"\"Returns an API client for the YouTube Data API.\n",
        "\n",
        "    e.g. youtube_client = connect_to_youtube()\n",
        "\n",
        "    Pass this to any function that has a parameter called youtube_client.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return googleapiclient.discovery.build(\n",
        "        \"youtube\", \"v3\", developerKey=prompt_unset_env(\"YOUTUBE_API_KEY\", \"Enter your YouTube API Key:\")\n",
        "    )\n",
        "\n",
        "\n",
        "def connect_to_reddit():\n",
        "    \"\"\"Returns an API client for the Reddit API\n",
        "\n",
        "    e.g. reddit_client = connect_to_reddit()\n",
        "\n",
        "    Pass this to any function that has a paremeter called reddit_client.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return praw.Reddit(\n",
        "        client_id=prompt_unset_env(\"REDDIT_CLIENT_ID\", \"Enter your Reddit client ID:\"),\n",
        "        client_secret=prompt_unset_env(\"REDDIT_CLIENT_SECRET\", \"Enter your Reddit client secret:\"),\n",
        "        username=prompt_unset_env(\"REDDIT_USERNAME\", \"Enter your Reddit username:\"),\n",
        "        password=prompt_unset_env(\"REDDIT_PASSWORD\", \"Enter your Reddit account password (the one you use to sign in):\"),\n",
        "        user_agent=\"COMP 540 Assignment 4\",\n",
        "    )\n",
        "\n",
        "def prompt_unset_env(env_var: str, prompt: str):\n",
        "    \"\"\"\n",
        "    Asks the user for env_var with the text prompt\n",
        "    if os.environ[env_var] is unset.\n",
        "\n",
        "    You should not need to call this function directly.\n",
        "    \"\"\"\n",
        "    if env_var not in os.environ:\n",
        "        os.environ[env_var] = getpass(prompt)\n",
        "    return os.environ[env_var]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieg1TFjab3JF"
      },
      "source": [
        "### Fetch YouTube data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T-vA_xlb3JF"
      },
      "source": [
        "#### Get most popular videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMLbQLWOb3JF"
      },
      "source": [
        "The code in the cell below fetches the top \"Trending\" videos across YouTube. Use it to get a cross-section of the full range of topics discussed on YouTube.\n",
        "\n",
        "Additionally, YouTube maintains a list of 30 or so \"Categories\", into which it sorts every video on the site. We can use them to limit the types of videos that the \"Trending\" list returns. For example, if we only want \"News & Politics\", we can call get_most_popular_youtube_videos(youtube_client, \"25\")\n",
        "\n",
        "Provided is a function that gets the top n posts for the given category ID. In the docstring, there is a description of how to use the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn4atuVXb3JF"
      },
      "outputs": [],
      "source": [
        "YOUTUBE_CATEGORIES = {\n",
        "    'Film & Animation': \"1\",\n",
        "    'Autos & Vehicles': \"2\",\n",
        "    'Music': \"10\",\n",
        "    'Pets & Animals': \"15\",\n",
        "    'Sports': \"17\",\n",
        "    'Short Movies': \"18\",\n",
        "    'Travel & Events': \"19\",\n",
        "    'Gaming': \"20\",\n",
        "    'Videoblogging': \"21\",\n",
        "    'People & Blogs': \"22\",\n",
        "    'Comedy': \"34\",\n",
        "    'Entertainment': \"24\",\n",
        "    'News & Politics': \"25\",\n",
        "    'Howto & Style': \"26\",\n",
        "    'Education': \"27\",\n",
        "    'Science & Technology': \"28\",\n",
        "    'Nonprofits & Activism': \"29\",\n",
        "    'Movies': \"30\",\n",
        "    'Anime/Animation': \"31\",\n",
        "    'Action/Adventure': \"32\",\n",
        "    'Classics': \"33\",\n",
        "    'Documentary': \"35\",\n",
        "    'Drama': \"36\",\n",
        "    'Family': \"37\",\n",
        "    'Foreign': \"38\",\n",
        "    'Horror': \"39\",\n",
        "    'Sci-Fi/Fantasy': \"40\",\n",
        "    'Thriller': \"41\",\n",
        "    'Shorts': \"42\",\n",
        "    'Shows': \"43\",\n",
        "    'Trailers': \"44\"\n",
        "}\n",
        "\n",
        "def get_most_popular_youtube_videos(youtube_client, category_id: str | None = None, n_videos: int=50, region_code: str = \"US\"):\n",
        "  \"\"\"\n",
        "  Lists the n most popular videos on YouTube in the US for the given category ID.\n",
        "  Videos are output as a raw dictionary.\n",
        "\n",
        "  Parameters:\n",
        "  - youtube_client: YouTube API client, retrieved from connect_to_youtube().\n",
        "  - category_id: The Category ID to fetch the videos from. Categories & their associated IDs can be found in the YOUTUBE_CATEGORIES dictionary\n",
        "  - n_videos: Number of videos to return. YouTube will return up to 175. Defaults to 50.\n",
        "  - region_code: Region to pull videos from. Defaults to \"US\".\n",
        "\n",
        "  Example:\n",
        "    youtube_client = connect_to_youtube()\n",
        "    category_id = YOUTUBE_CATEGORIES[\"News & Politics\"]\n",
        "\n",
        "    videos = get_most_popular_US_youtube_videos(youtube_client, category_id, n_videos=50)\n",
        "\n",
        "    videos_df = create_videos_df(videos)\n",
        "  )\n",
        "  \"\"\"\n",
        "\n",
        "  result = []\n",
        "  nextPageToken = None\n",
        "  while n_videos > 0:\n",
        "    response = youtube_client.videos().list(\n",
        "      part=\"snippet\",\n",
        "      chart=\"mostPopular\",\n",
        "      maxResults=min(n_videos, 50),\n",
        "      regionCode=region_code,\n",
        "      videoCategoryId=category_id,\n",
        "      pageToken=nextPageToken\n",
        "    ).execute()\n",
        "    result.extend(response[\"items\"])\n",
        "    if \"nextPageToken\" not in response:\n",
        "      break\n",
        "    nextPageToken = response[\"nextPageToken\"]\n",
        "    n_videos -= 50\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgBjyN-Ub3JF"
      },
      "source": [
        "#### Search YouTube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLEyx_Jpb3JF"
      },
      "source": [
        "This function performs a search of the YouTube website - it effectively types something into the YouTube search bar with code.\n",
        "\n",
        "You can use it to get results more specific to a topic of interest of yours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO5FfcM8b3JF"
      },
      "outputs": [],
      "source": [
        "def search_youtube(youtube_client, query: str, n_videos: int=50, region_code: str=\"US\", order=\"relevance\"):\n",
        "    \"\"\"Perform a search of the YouTube site.\n",
        "\n",
        "    Parameters:\n",
        "        youtube_client: YouTube API client, retrieved from connect_to_youtube().\n",
        "        query: The query to search. Think of it as like typing into the YouTube search bar.\n",
        "        n_videos: Number of videos to return. Defaults to 50.\n",
        "        region_code: Region to search in. Defaults to \"US\"\n",
        "        order: Order to return videos in. Defaults to \"relevance\". Other valid values are as follows:\n",
        "            date - Resources are sorted in reverse chronological order based on the date they were created.\n",
        "            rating - Resources are sorted from highest to lowest rating.\n",
        "            relevance - Resources are sorted based on their relevance to the search query. This is the default value for this parameter.\n",
        "            title - Resources are sorted alphabetically by title.\n",
        "            videoCount - Channels are sorted in descending order of their number of uploaded videos.\n",
        "            viewCount - Resources are sorted from highest to lowest number of views. For live broadcasts, videos are sorted by number of concurrent viewers while the broadcasts are ongoing.\n",
        "\n",
        "    Example:\n",
        "        youtube_client = connect_to_youtube()\n",
        "\n",
        "        videos_about_harris = search_youtube(youtube_client, \"Kamala Harris\")\n",
        "\n",
        "        videos_about_harris_df = create_videos_df(videos_about_harris)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    result = []\n",
        "    nextPageToken = None\n",
        "    while n_videos > 0:\n",
        "        response = youtube_client.search().list(\n",
        "            part=\"snippet\",\n",
        "            q=query,\n",
        "            order=order,\n",
        "            maxResults=min(n_videos, 50),\n",
        "            regionCode=region_code,\n",
        "            pageToken=nextPageToken,\n",
        "            type=\"video\"\n",
        "        ).execute()\n",
        "        result.extend(response[\"items\"])\n",
        "        if \"nextPageToken\" not in response:\n",
        "            break\n",
        "        nextPageToken = response[\"nextPageToken\"]\n",
        "        n_videos -= 50\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDeWY8yb3JF"
      },
      "source": [
        "#### Get YouTube comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1siQH1zjb3JF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_youtube_comments(youtube_client, video_id: str, n_comments: int=100):\n",
        "  \"\"\"Fetches the top n comments that are replies to the given YouTube video ID.\n",
        "\n",
        "  Parameters:\n",
        "    - youtube_client: YouTube API client, retrieved from connect_to_youtube().\n",
        "    - video ID: The video ID to get the comments for\n",
        "\n",
        "  Example:\n",
        "\n",
        "  youtube_client = connect_to_youtube()\n",
        "  example_video = search_youtube(youtube_client, \"Donald Trump\")[0]\n",
        "  example_video_id = get_youtube_video_id(example_video)\n",
        "\n",
        "  comments = get_youtube_comments(youtube_client, example_video_id)\n",
        "  \"\"\"\n",
        "\n",
        "  result = []\n",
        "  nextPageToken = None\n",
        "  while n_comments > 0:\n",
        "    response =  youtube_client.commentThreads().list(\n",
        "      part=\"snippet,replies\",\n",
        "      maxResults=min(n_comments, 100),\n",
        "      order=\"relevance\",\n",
        "      pageToken=nextPageToken,\n",
        "      videoId=video_id\n",
        "    ).execute()\n",
        "    result.extend(response[\"items\"])\n",
        "    # Fewer than 100 responses: return early\n",
        "    if nextPageToken not in response:\n",
        "      return result\n",
        "    nextPageToken = response[\"nextPageToken\"]\n",
        "    n_comments -= 100\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DJeRu5Nb3JF"
      },
      "source": [
        "#### Format YouTube data to Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xsLxFfbb3JF"
      },
      "outputs": [],
      "source": [
        "def create_youtube_videos_df(videos) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "    Takes in the given videos from the YouTube API.\n",
        "    Using them, creates the following dataframe:\n",
        "\n",
        "    Columns:\n",
        "    - id: YouTube video ID (index)\n",
        "    - raw: Raw output from YouTube API\n",
        "    - title: Video title\n",
        "    - channel: Channel that posted the video\n",
        "\n",
        "    Example:\n",
        "\n",
        "      videos = get_most_popular_youtube_videos(<...>)\n",
        "\n",
        "      videos_df = create_videos_df(youtube_client, videos)\n",
        "  \"\"\"\n",
        "  # Videos dataframe\n",
        "  df = pd.DataFrame(\n",
        "    data={\"raw\": videos}\n",
        "  )\n",
        "  df[\"id\"] = df.raw.apply(get_youtube_video_id)\n",
        "  df.set_index(\"id\", inplace=True)\n",
        "  df[\"title\"] = df.raw.apply(get_youtube_video_title)\n",
        "  df[\"channel\"] = df.raw.apply(get_youtube_video_channel)\n",
        "  return df\n",
        "\n",
        "def create_youtube_comments_df(youtube_client, videos, n_per_video: int=100) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  \"\"\"\n",
        "   Takes in the given videos from the YouTube API.\n",
        "   Fetches n_per_video commments for each video, and formats them\n",
        "   into the following dataframe:\n",
        "\n",
        "   Columns:\n",
        "   - id: YouTube comment ID (index)\n",
        "   - raw: Raw output from YouTube API\n",
        "   - body: Comment text\n",
        "   - author: Author that posted the comment\n",
        "   - likes: Like count of comment\n",
        "   - parent_id: ID of the comment that this comment is a reply to. pd.NA if it is a top-level comment\n",
        "\n",
        "   Example:\n",
        "\n",
        "    videos = get_most_popular_youtube_videos(<...>)\n",
        "\n",
        "    comments_df = create_comments_df(youtube_client, videos, 100)\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "  if isinstance(videos, pd.DataFrame):\n",
        "    videos = [video[\"raw\"] for video in videos]\n",
        "\n",
        "  # Comments dataframe\n",
        "  comments = {\"raw\": [], \"video_id\": []}\n",
        "  for video in videos:\n",
        "    video_id = get_youtube_video_id(video)\n",
        "    try:\n",
        "      video_comments = get_youtube_comments(youtube_client, video_id, n_comments=n_per_video)\n",
        "    except googleapiclient.errors.HttpError:\n",
        "      continue\n",
        "    comments[\"raw\"].extend(video_comments)\n",
        "    comments[\"video_id\"].extend([video_id] * len(video_comments))\n",
        "\n",
        "  youtube_comments_df = pd.DataFrame(\n",
        "    data=comments\n",
        "  )\n",
        "  youtube_comments_df[\"id\"] = youtube_comments_df.raw.apply(get_youtube_comment_id)\n",
        "  youtube_comments_df.set_index(\"id\", inplace=True)\n",
        "  youtube_comments_df[\"body\"] = youtube_comments_df.raw.apply(get_youtube_comment_body)\n",
        "  youtube_comments_df[\"author\"] = youtube_comments_df.raw.apply(get_youtube_comment_author)\n",
        "  youtube_comments_df[\"likes\"] = youtube_comments_df.raw.apply(get_youtube_comment_likes)\n",
        "\n",
        "\n",
        "  # Add replies to comments dataframe\n",
        "  replies_raw = []\n",
        "  for comment in comments[\"raw\"]:\n",
        "    for reply in get_youtube_comment_replies(comment):\n",
        "      replies_raw.append(reply)\n",
        "\n",
        "  replies_df = pd.DataFrame(\n",
        "    data={\"raw\": replies_raw}\n",
        "  )\n",
        "  replies_df[\"id\"] = replies_df.raw.apply(lambda x: x[\"id\"])\n",
        "  replies_df.set_index(\"id\", inplace=True)\n",
        "  replies_df[\"video_id\"] = replies_df.raw.apply(lambda x: x[\"snippet\"][\"videoId\"])\n",
        "  replies_df[\"parent_id\"] = replies_df.raw.apply(lambda x: x[\"snippet\"][\"parentId\"])\n",
        "  replies_df[\"body\"] = replies_df.raw.apply(get_youtube_comment_reply_body)\n",
        "  replies_df[\"author\"] = replies_df.raw.apply(get_youtube_comment_reply_author)\n",
        "  replies_df[\"likes\"] = replies_df.raw.apply(get_youtube_comment_reply_likes)\n",
        "  youtube_comments_df = pd.concat([youtube_comments_df, replies_df])\n",
        "\n",
        "  return youtube_comments_df\n",
        "\n",
        "def join_youtube_dfs(youtube_videos_df, youtube_comments_df):\n",
        "  \"\"\"Joins together the YouTube comments and posts.\n",
        "  \"\"\"\n",
        "  youtube_comments_parents_joined = youtube_comments_df.join(youtube_comments_df, on=\"parent_id\", rsuffix=\"_parent\")\n",
        "  return youtube_comments_parents_joined.join(youtube_videos_df, on=\"video_id\", lsuffix=\"_comment\", rsuffix=\"_video\")\n",
        "\n",
        "\n",
        "# The following functions are used by the create_youtube_df\n",
        "# functions to extract data from YouTube. You likely won't use them directly.\n",
        "\n",
        "def get_youtube_video_id(video):\n",
        "  if isinstance(video[\"id\"], str):\n",
        "    return video[\"id\"]\n",
        "  else:\n",
        "    return video[\"id\"][\"videoId\"]\n",
        "\n",
        "def get_youtube_video_title(video):\n",
        "  return video[\"snippet\"][\"title\"]\n",
        "\n",
        "def get_youtube_video_channel(video):\n",
        "  return video[\"snippet\"][\"channelTitle\"]\n",
        "\n",
        "def get_youtube_comment_body(comment):\n",
        "  return comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "\n",
        "def get_youtube_comment_replies(comment):\n",
        "  try:\n",
        "    return comment[\"replies\"][\"comments\"]\n",
        "  except KeyError:\n",
        "    # No replies field in the comment... there must not be any replies.\n",
        "    return []\n",
        "\n",
        "def get_youtube_comment_reply_body(comment):\n",
        "  return comment[\"snippet\"][\"textDisplay\"]\n",
        "\n",
        "def get_youtube_comment_author(comment):\n",
        "  return comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
        "\n",
        "def get_youtube_comment_reply_author(comment):\n",
        "  return comment[\"snippet\"][\"authorDisplayName\"]\n",
        "\n",
        "def get_youtube_comment_likes(comment):\n",
        "  return comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
        "\n",
        "def get_youtube_comment_reply_likes(comment):\n",
        "  return comment[\"snippet\"][\"likeCount\"]\n",
        "\n",
        "def get_youtube_comment_id(comment):\n",
        "  return comment[\"id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41JI4x2Tb3JF"
      },
      "source": [
        "### Fetch Reddit data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or12X4oJb3JF"
      },
      "source": [
        "#### Get posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cogQEqGJb3JG"
      },
      "outputs": [],
      "source": [
        "def search_reddit(reddit_client: praw.Reddit, query: str, n_posts: int=25, subreddit: str=\"all\", time_filter=\"day\"):\n",
        "    \"\"\"\n",
        "    Performs a search of the Reddit site for query. This is like typing query into the search bar at the top of the site.\n",
        "\n",
        "    Inputs:\n",
        "    - reddit_client: The API client returned from connect_to_reddit()\n",
        "    - query: The string to search the site for\n",
        "    - n_posts: Number of posts to return. Defaults to 25\n",
        "    - subreddit: The subreddit to search, without the /r/ prefix (e.g. to search /r/politics, should be \"politics\").\n",
        "        Defaults to /r/all, which is a fake subreddit that displays all posts across the site.\n",
        "    - time_filter: Limit your search to posts created within a certain time range.\n",
        "        Can be one of: \"all\", \"day\", \"hour\", \"month\", \"week\", or \"year\" (default: \"day\")\n",
        "    \"\"\"\n",
        "    return list(reddit_client.subreddit(subreddit).search(query=query, time_filter=time_filter, limit=n_posts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSw5YwXRb3JG"
      },
      "outputs": [],
      "source": [
        "def get_most_popular_reddit_posts(reddit_client: praw.Reddit, n_posts: int=25, subreddit: str=\"all\", time_filter=\"day\"):\n",
        "    \"\"\"\n",
        "    Gets the most upvoted posts on Reddit.\n",
        "\n",
        "    Inputs:\n",
        "    - reddit_client: The API client returned from connect_to_reddit()\n",
        "    - n_posts: Number of posts to return. Defaults to 25\n",
        "    - subreddit: The subreddit to return posts from, without the /r/ prefix (e.g. to search /r/politics, should be \"politics\").\n",
        "        Defaults to /r/all, which is a fake subreddit that displays all posts across the site.\n",
        "    - time_filter: Limit to posts created within a certain time range.\n",
        "        Can be one of: \"all\", \"day\", \"hour\", \"month\", \"week\", or \"year\" (default: \"day\")\n",
        "    \"\"\"\n",
        "    return list(reddit_client.subreddit(subreddit).top(limit=n_posts, time_filter=time_filter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4yo7BWBb3JG"
      },
      "source": [
        "#### Format Reddit data to Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMd7R6ahb3JG"
      },
      "outputs": [],
      "source": [
        "def create_reddit_posts_df(posts: Iterable[praw.models.Submission]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Takes in an iterable (list, generator, set, etc) of Reddit posts and returns the following DataFrame:\n",
        "\n",
        "    Columns:\n",
        "        id: Post ID (index)\n",
        "        likes: Upvote / downvote count of post\n",
        "        title: Title of post\n",
        "        body: Text of post\n",
        "        subreddit: Subreddit where post was posted\n",
        "        author: Username that wrote post\n",
        "        published_timestamp: Time when the post was published.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    post_data = {\n",
        "        \"id\": [],\n",
        "        \"likes\": [],\n",
        "        \"title\": [],\n",
        "        \"body\": [],\n",
        "        \"subreddit\": [],\n",
        "        \"author\": [],\n",
        "        \"published_timestamp\": []\n",
        "    }\n",
        "\n",
        "    for submission in posts:\n",
        "        post_data[\"id\"].append(submission.id)\n",
        "        post_data[\"published_timestamp\"].append(submission.created)\n",
        "        post_data[\"author\"].append(submission.author)\n",
        "        post_data[\"likes\"].append(submission.score)\n",
        "        post_data[\"body\"].append(submission.selftext)\n",
        "        post_data[\"title\"].append(submission.title)\n",
        "        post_data[\"subreddit\"].append(submission.subreddit.display_name)\n",
        "\n",
        "\n",
        "    return pd.DataFrame(post_data).set_index(\"id\")\n",
        "\n",
        "def create_reddit_comments_df(posts: Iterable[praw.models.Submission], comment_limit=10) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Takes in an iterable (list, generator, set, etc) of Reddit posts and returns a DataFrame\n",
        "    containing data about the comments.\n",
        "\n",
        "    Columns:\n",
        "        id: Comment ID (index)\n",
        "        post_id: ID of the post this comment is a reply to\n",
        "        likes: Upvote / downvote count of commment\n",
        "        body: Text of comment\n",
        "        subreddit: Subreddit where post was posted\n",
        "        author: Username that wrote comment\n",
        "        order_returned: 0 if 1st comment retrieved from the post, 1 if 2nd comment, etc\n",
        "    \"\"\"\n",
        "    comment_data = {\n",
        "        \"id\": [],\n",
        "        \"post_id\": [],\n",
        "        \"likes\": [],\n",
        "        \"author\": [],\n",
        "        \"body\": [],\n",
        "        \"author\": [],\n",
        "        \"order_returned\": [],\n",
        "    }\n",
        "    for submission in posts:\n",
        "        submission.comments.replace_more(limit=0)\n",
        "        for idx, comment in enumerate(submission.comments):\n",
        "            if idx >= comment_limit:\n",
        "                break\n",
        "            comment_data[\"order_returned\"].append(idx)\n",
        "            comment_data[\"id\"].append(comment.id)\n",
        "            comment_data[\"post_id\"].append(submission.id)\n",
        "            comment_data[\"likes\"].append(comment.score)\n",
        "            comment_data[\"author\"].append(comment.author)\n",
        "            comment_data[\"body\"].append(comment.body)\n",
        "    return pd.DataFrame(comment_data).set_index(\"id\")\n",
        "\n",
        "def join_reddit_dfs(reddit_posts_df, reddit_comments_df):\n",
        "    \"\"\"Joins together the comments and posts.\"\"\"\n",
        "    return reddit_comments_df.join(reddit_posts_df, on=\"post_id\", rsuffix=\"_post\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7AQGagKb3JG"
      },
      "source": [
        "### Put it all together and collect the data (MODIFY THIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXSbs4DBb3JG"
      },
      "source": [
        "This code takes all of the utility functions for data collection in the cells above and uses them to collect the data we want from YouTube and Reddit.\n",
        "\n",
        "Modify this to get data relevant to topics you are interested in. We've provided you with code that, on each site:\n",
        "1. Performs searches for Kamala Harris and Donald Trump\n",
        "2. Pulls in the top 5 most popular posts sitewide\n",
        "3. Pulls in the top 5 most popular political posts on each site\n",
        "\n",
        "You might consider:\n",
        "1. Modifying the search keywords or adding more\n",
        "2. Pulling in a larger number of the most popular posts sitewide\n",
        "3. Analyzing different categories (YouTube) or subreddits (Reddit)\n",
        "\n",
        "You should aim for a data collection method that:\n",
        "1. Filters out non-political content - we don't want to waste our time running inference on this!\n",
        "2. Gets a representative sample of the users of the site - we want to gauge what everyone thinks, not just one small community of users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMeFN7UHb3JG",
        "outputId": "7d23ae3a-3812-40a7-d332-6f01ab583b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Reddit client ID:··········\n",
            "Enter your Reddit client secret:··········\n",
            "Enter your Reddit username:··········\n",
            "Enter your Reddit account password (the one you use to sign in):··········\n",
            "Enter your YouTube API Key:··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        }
      ],
      "source": [
        "# Connect clients\n",
        "reddit_client = connect_to_reddit()\n",
        "youtube_client = connect_to_youtube()\n",
        "\n",
        "# NOTE: For each query, we only get 5 posts.\n",
        "# This is so that, when you run this cell for the first time,\n",
        "# it completes in a reasonable amount of time.\n",
        "# We will almost certainly want far more data than this!\n",
        "# MODIFY THIS PART OF THE CODE\n",
        "\n",
        "# We search for Harris and Trump - are there more interesting things you can search for?\n",
        "harris_reddit_posts = search_reddit(reddit_client, \"Kamala Harris\", n_posts=5)\n",
        "trump_reddit_posts = search_reddit(reddit_client, \"Donald Trump\", n_posts=5)\n",
        "# We get the 5 most upvoted posts sitewide for the day - should we get more? Less?\n",
        "popular_reddit_posts = get_most_popular_reddit_posts(reddit_client, n_posts=5)\n",
        "# We check /r/politics for the most popular posts as well. Are there other subreddits you are\n",
        "# interested in analyzing a large number of posts from?\n",
        "# https://www.reddit.com/best/communities/1/ is a good place to find interesting subreddits.\n",
        "popular_reddit_posts_on_r_politics = get_most_popular_reddit_posts(reddit_client, n_posts=5, subreddit=\"politics\")\n",
        "\n",
        "posts_combined = harris_reddit_posts + trump_reddit_posts + popular_reddit_posts + popular_reddit_posts_on_r_politics\n",
        "\n",
        "# END MODIFY THIS PART OF THE CODE\n",
        "\n",
        "reddit_posts_df = create_reddit_posts_df(posts_combined)\n",
        "reddit_comments_df = create_reddit_comments_df(posts_combined, comment_limit=10) # do we want to fetch more comments?\n",
        "reddit_df = join_reddit_dfs(reddit_posts_df, reddit_comments_df)\n",
        "\n",
        "\n",
        "# Get the top 50 most popular YouTube videos for the News & Politics category\n",
        "# Also search for Harris and Trump\n",
        "\n",
        "harris_youtube_videos = search_youtube(youtube_client, \"Kamala Harris\", n_videos=5)\n",
        "trump_youtube_videos = search_youtube(youtube_client, \"Donald Trump\", n_videos=5)\n",
        "popular_youtube_videos_sitewide = get_most_popular_youtube_videos(youtube_client, n_videos=5)\n",
        "# Are there other interesting category IDs to analyze?\n",
        "category_id = YOUTUBE_CATEGORIES[\"News & Politics\"]\n",
        "popular_youtube_politics_videos = get_most_popular_youtube_videos(youtube_client, category_id, n_videos=5)\n",
        "\n",
        "videos_combined = harris_youtube_videos + trump_youtube_videos + popular_youtube_videos_sitewide + popular_youtube_politics_videos\n",
        "\n",
        "youtube_videos_df = create_youtube_videos_df(videos_combined)\n",
        "youtube_comments_df = create_youtube_comments_df(youtube_client, videos_combined, n_per_video=2) # do we want to fetch more comments?\n",
        "youtube_df = join_youtube_dfs(youtube_videos_df, youtube_comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1hoxBbxfsv8"
      },
      "outputs": [],
      "source": [
        "# Should you accidentally enter the wrong key for anything, run the corresponding line below:\n",
        "# del os.environ[\"YOUTUBE_API_KEY\"]\n",
        "# del os.environ[\"REDDIT_CLIENT_ID\"]\n",
        "# del os.environ[\"REDDIT_CLIENT_SECRET\"]\n",
        "# del os.environ[\"REDDIT_USERNAME\"]\n",
        "# del os.environ[\"REDDIT_PASSWORD\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8csO_ob3JG"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVCFpcdub3JG"
      },
      "source": [
        "We'll be using the newly released [Meta Llama 3.2 Instruct model with 3 billion parameters](https://huggingface.co/meta-llama/Llama-3.2-3B) for our analysis. This model is incredibly powerful for how small it is, and, most importantly, extremely fast.\n",
        "\n",
        "Run the below to log in to HuggingFace and load the model onto your GPU, after you have completed the instructions in the **Guide to API Access**. Enter your token when prompted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "7gHYW8fQb3JG"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = prompt_unset_env(\"HF_TOKEN\", \"Enter your HuggingFace token:\")\n",
        "HF_CACHE = os.environ.get(\"HF_CACHE\", None)\n",
        "\n",
        "HF_MODEL_NAME=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Here we compress our model with 8-bit \"quantization\".\n",
        "# You can think of quantization as rounding the model's\n",
        "# parameters, which are stored as 32-bit floats, to 8 bits.\n",
        "# This saves a lot of space on our GPU!\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "  load_in_8bit=True,\n",
        "  bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  HF_MODEL_NAME,\n",
        "  quantization_config=quantization_config,\n",
        "  cache_dir=HF_CACHE\n",
        ")\n",
        "\n",
        "# Notice the 'Causal' ain the constructor.\n",
        "# Llama is a 'Causal' LM, meaning it predicts the next token given only the previous tokens.\n",
        "# Some 'masked' LMs like BERT can predict a token in the middle of a sentence.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  HF_MODEL_NAME,\n",
        "  quantization_config=quantization_config,\n",
        "  # Let HuggingFace decide which device to put our model on.\n",
        "  # This will efficiently share CPU and GPU resources.\n",
        "  device_map=\"auto\",\n",
        "  token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# REPLACE MODEL LOAD CODE ABOVE WITH THE FOLLOWING IF FINE-TUNING (OPTIONAL)\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#   \"lora\",\n",
        "#   cache_dir=HF_CACHE,\n",
        "#   quantization_config=quantization_config,\n",
        "#   device_map=\"auto\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EuVyRBXb3JH"
      },
      "source": [
        "## Prompt Format for Instruct models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOhEUPbUb3JH"
      },
      "source": [
        "The LLMs that you have studied so far simply predict the most likely next token based on the previous tokens in the sequence. These models do not lend themselves well to instruction-following. For example, in the training corpus of a normal LLM, \"Write a letter to my mom\" is not likely to precede the text \"Dear mom\". So, it is more likely to complete the sentence with text like \"for Mother's day\".\n",
        "\n",
        "Instruct-tuned models have gone through an extra training step where they are fed a series of instructions paired with desired outputs. For example, as far as we are aware, Meta Llama was trained on a corpus of text which looked like this:\n",
        "\n",
        "  \n",
        "  > <|begin_of_text|>\n",
        "  >\n",
        "  > <|start_header_id|>system<|end_header_id|>\n",
        "  >\n",
        "  > You will be given a question. Answer it like you are a cowboy.\n",
        "  >\n",
        "  > <|eot_id|>\n",
        "  >\n",
        "  > <|start_header_id|>user<|end_header_id|>\n",
        "  >\n",
        "  > What is a polite way to say hello to your friend?\n",
        "  >\n",
        "  > <|eot_id|>\n",
        "  >\n",
        "  > <|start_header_id|>assistant<|end_header_id|>\n",
        "  >\n",
        "  > Howdy, partner!\n",
        "  >\n",
        "  > <|eos_token|>\n",
        "\n",
        "This is documented in Meta Llama's [Model Card](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
        "\n",
        "The text under `<|start_header_id|>system<|end_header_id|>` is known as the **system prompt**. The attention layer of the model is trained such that text in the system prompt always has a high weight. It is useful for information that the model needs to not forget, even through a long prompt.\n",
        "\n",
        "The text under `<|start_header_id|>user<|end_header_id|>` is the **user prompt**, which receives no special treatment.\n",
        "\n",
        "The text under `<|start_header_id|>assistant<|end_header_id|>` is the text that the model is supposed to generate.\n",
        "\n",
        "These Instruct models will be more useful to us, since we want them to perform complex instructions rather than simple text completions. The function below formats text into a system and user prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HVawTekb3JK"
      },
      "outputs": [],
      "source": [
        "LLAMA_INSTRUCT_PROMPT_FORMAT = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "  {system_prompt}<|eot_id|>\n",
        "  <|start_header_id|>user<|end_header_id|>\n",
        "  {user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\" # End after assistant header so that the first token predicted will be the model's response.\n",
        "\n",
        "def format_instruct(system_prompt: str, user_prompt: str):\n",
        "  \"\"\"Formats the system prompt and the user prompt into a single string usable by meta llama.\"\"\"\n",
        "  return LLAMA_INSTRUCT_PROMPT_FORMAT.format(\n",
        "    system_prompt=system_prompt,\n",
        "    user_prompt=user_prompt\n",
        "  )\n",
        "def get_output_from_instruct(output_raw: str):\n",
        "  \"\"\"If an LLM was prompted from the Instruct format, get the text that was generated\n",
        "  by the model. (specifically, between the assistant header and the EOS token)\"\"\"\n",
        "  return output_raw.split(\"assistant<|end_header_id|>\")[1] \\\n",
        "    .split(tokenizer.eos_token)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTalnZLib3JK"
      },
      "source": [
        "\n",
        "## Inference Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q7iNKl7b3JL"
      },
      "source": [
        "This code is used to run prompts through the model. It takes in a batch (group) of prompts as strings and runs inference on them.\n",
        "\n",
        "This particular function is important to understand, so here is a breakdown:\n",
        "\n",
        "- 1 ) The prompt batch is tokenized (converted to tensor embeddings) using the tokenizer\n",
        "    - 1.1 ) All items in the batch must be the same length.\n",
        "        This allows for high-speed parallel processing of the batches on the GPU.\n",
        "        In order to make the prompts the same length,\n",
        "        we can add \"padding\" tokens which the model will ignore.\n",
        "        In this case, we add EOS tokens on the left until all\n",
        "        prompts are the same length. We add them on the left\n",
        "        instead of the right so that there isn't an EOS token between\n",
        "        the prompt and the model's output\n",
        "- 2 ) The token embeddings are run through the model's forward function\n",
        "    - 2.1 ) An upper limit on the number of new tokens is set. This is a failsafe in case the model gets stuck in a loop and fails to ever generate an EOS token. Without it, generation would just carry on forever!\n",
        "- 3 ) The token output of the model's forward function is decoded back into strings using the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OJBt5YCb3JL"
      },
      "outputs": [],
      "source": [
        "def inference(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt_batch: list[str], max_new_tokens: int=196, verbose: bool=False) -> list[str]:\n",
        "  \"\"\"Tokenizes prompt_batch using tokenizer and runs them through model all at once.\n",
        "  Returns the output from the model as a list of strings, where inference(...)[i] == output from prompt_batch[i]\n",
        "\n",
        "  Parameters:\n",
        "    - model: LLM to run inference on (loaded from HuggingFace)\n",
        "    - tokenizer: Tokenizer to encode inputs with (loaded from HuggingFace)\n",
        "    - prompt_batch: Batch of prompts to run inference on\n",
        "    - max_new_tokens: Maximum number of new tokens to generate\n",
        "    - verbose: Set to True for additional print messages\n",
        "\n",
        "  Example:\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(...)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(...)\n",
        "\n",
        "    output = inference(model, tokenizer, [\"one plus one equals\"]) // [\"<|bos_token|>one plus one equals two<|eot_id|>\"]\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "      # 1) Encode prompts as tensors with the tokenizer\n",
        "      # 1.1) Add padding\n",
        "      tokenizer.pad_token = tokenizer.eos_token\n",
        "      tokenizer.padding_side = \"left\"\n",
        "      # Now, tokenize the inputs\n",
        "      inputs = tokenizer(\n",
        "        prompt_batch,\n",
        "        return_tensors = \"pt\",\n",
        "\n",
        "        padding=True,\n",
        "      )\n",
        "      if verbose:\n",
        "        print(\"In token shape\", inputs[\"input_ids\"].shape)\n",
        "        print(\"Memory usage before moving tensors to CUDA: \", torch.cuda.memory_summary())\n",
        "\n",
        "      # Move the inputs to the same device as the model.\n",
        "      inputs = inputs.to(model.device)\n",
        "\n",
        "      if verbose:\n",
        "        print(\"Memory usage after moving tensors to CUDA: \", torch.cuda.memory_summary())\n",
        "\n",
        "      # 2) Generate output tokens\n",
        "      outputs = model.generate(\n",
        "        **inputs,\n",
        "        # 2.1) Set an upper bound on the number of new tokens\n",
        "        max_new_tokens=max_new_tokens\n",
        "      )\n",
        "      if verbose:\n",
        "        print(\"Out token shape\", outputs.shape)\n",
        "        print(\"Memory usage after inference moving to device: \", torch.cuda.memory_summary())\n",
        "      # 3) Decode the output tokens back into strings.\n",
        "      return tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXVmH-sb3JL"
      },
      "source": [
        "### Other inference code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W87rywnDb3JL"
      },
      "source": [
        "#### Code for batching prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRRAGNkkb3JL"
      },
      "source": [
        "The description of the previous function mentioned running prompts in batches. Batching prompts has a significant (up to 10x) speed improvement because:\n",
        "\n",
        "1) It allows for many prompts to be processed in parallel rather than one at a time\n",
        "2) If 2 prompts in a batch share a prefix, the model will cache the K and V self-attention components for those prefixes so that they are only computed once across the whole batch. This is useful when we are processing hundreds of prompts with the same system prompt.\n",
        "\n",
        "However, the entire batch needs to be moved over to the GPU at once, so there is an upper limit to how many tokens we can fit into a single batch.\n",
        "\n",
        "These functions break a list of prompts into batches, cramming as many as possible onto our GPU as possible. You likely won't call either of these functions directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBEDB05ib3JL"
      },
      "outputs": [],
      "source": [
        "def get_num_tokens(tokenizer: PreTrainedTokenizer, seq: str):\n",
        "  \"\"\"Returns the number of tokens in the sequence.\"\"\"\n",
        "  return tokenizer(seq, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
        "\n",
        "def maximum_size_batches(tokenizer, prompts: list[str], max_tokens_per_batch: int, max_new_tokens: int = 0) -> list[list[str]]:\n",
        "  \"\"\"\n",
        "  Separates a list of prompts into a list of lists, where each inner list is a batch.\n",
        "  The number of tokens in each batch will not exceed max_tokens_per_batch.\n",
        "  \"\"\"\n",
        "  batches = []\n",
        "  curr_batch = []\n",
        "  max_tok_count_curr_batch = float(\"-inf\")\n",
        "  for prompt in prompts:\n",
        "    prompt_tok_count = get_num_tokens(tokenizer, prompt)\n",
        "    max_tok_count_curr_batch = max(prompt_tok_count, max_tok_count_curr_batch)\n",
        "    if (max_tok_count_curr_batch + max_new_tokens) * (len(curr_batch) + 1) > max_tokens_per_batch:\n",
        "      batches.append(curr_batch)\n",
        "      curr_batch = []\n",
        "      max_tok_count_curr_batch = prompt_tok_count\n",
        "    curr_batch.append(prompt)\n",
        "  batches.append(curr_batch)\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRVCyGIFb3JL"
      },
      "source": [
        "#### Pulling it all together: Code to run inference on batches in a loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JKqGWZb3JL"
      },
      "source": [
        "This function puts all of the pieces together. It takes in a Pandas dataframe with the explained column format, and runs inference on each row, batching together the prompts. You will likely call it yourself to run inference. Later on, there is an example wheere we have done so which you can follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GSSQeZAb3JL"
      },
      "outputs": [],
      "source": [
        "# Only process as many tokens per batch as will fit on the GPU.\n",
        "# On CoLab, this is unfortunately only about 10,000.\n",
        "# The VM can handle up to 40,000!\n",
        "MAX_TOKENS_PER_BATCH=os.environ.get(\"MAX_TOKENS_PER_BATCH\", 10e3)\n",
        "\n",
        "def llama_instruct_inference_on_df_for_topic(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df: pd.DataFrame, topic: str, max_new_tokens: int=196):\n",
        "  \"\"\"\n",
        "  Using model and tokenizer, run inference on each row in the dataframe.\n",
        "  For each row, the user prompt is df[\"user_prompt\"],\n",
        "  and the system prompt is df[\"system_prompt {topic}\"].\n",
        "  Saves the inference result to df [\"inference_output {topic}\"].\n",
        "\n",
        "  Example:\n",
        "    youtube_df = join_youtube_dfs(...)\n",
        "\n",
        "    topic = \"Kamala Harris\"\n",
        "\n",
        "    youtube_df[\"user_prompt\"] =  youtube_df.apply(get_youtube_user_prompt, axis=1)\n",
        "    youtube_df[\"system_prompt \" + topic] = SENTIMENT_TOPICS_AND_PROMPTS[topic]\n",
        "\n",
        "    llama_instruct_inference_on_df_for_topic(model, tokenizer, topic)\n",
        "  \"\"\"\n",
        "  print(f\"Starting inference for topic {topic}\")\n",
        "  start = time.time()\n",
        "\n",
        "  df[f\"full_prompt {topic}\"] = df.apply(\n",
        "    lambda row: format_instruct(row[f\"system_prompt {topic}\"], row[\"user_prompt\"]),\n",
        "    axis=1\n",
        "  )\n",
        "\n",
        "  output = []\n",
        "  batches = maximum_size_batches(tokenizer, df[f\"full_prompt {topic}\"].to_list(), MAX_TOKENS_PER_BATCH, max_new_tokens)\n",
        "  print(\"Batch lengths\", [len(batch) for batch in batches])\n",
        "  for batch_num, batch in enumerate(batches):\n",
        "    print(\"Batch\", batch_num, \"Batch length\", len(batch))\n",
        "    output.extend(\n",
        "      get_output_from_instruct(x)\n",
        "      for x in inference(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        batch,\n",
        "        max_new_tokens\n",
        "      )\n",
        "    )\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  df[f\"inference_output {topic}\"] = output\n",
        "\n",
        "  print(\"Inference time: \", time.time() - start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyfojXzmb3JL"
      },
      "source": [
        "## Utility functions for inference postprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aNdFrqCb3JL"
      },
      "source": [
        "You don't need to understand these, unless you want to edit the output format in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh3lhKv7b3JL"
      },
      "outputs": [],
      "source": [
        "def extract_key_from_json_column(key):\n",
        "  \"\"\"Higher-order function.\n",
        "\n",
        "  Returns a function that, when applied to a Pandas series,\n",
        "  loads the series as JSON and extracts the given key from the JSON object.\n",
        "  \"\"\"\n",
        "  return lambda x: get_key_from_json_string(x, key)\n",
        "\n",
        "def get_key_from_json_string(json_string, key):\n",
        "  \"\"\"Return the value associated with key in the JSOn object represented in json_string.\n",
        "\n",
        "  If there is a missing end curly bracket, it will still be parsed.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    return json.loads(json_string)[key]\n",
        "  except json.decoder.JSONDecodeError:\n",
        "    # Try adding a closing bracket (model loves to forget this)\n",
        "    try:\n",
        "      return json.loads(json_string + \"}\")[key]\n",
        "    except:\n",
        "      print(\"Model output bad JSON\", json_string)\n",
        "      return pd.NA\n",
        "  except KeyError as e:\n",
        "    print(e)\n",
        "\n",
        "def extract_sentiment_from_inference_output(df: pd.DataFrame, topic: str):\n",
        "  \"\"\"\n",
        "  Given a dataframe that has been run through using llama_instruct_inference_on_df_for_topic,\n",
        "  extract the sentiment, explanation, and confidence scores from the output of the inference for topic\n",
        "  and place them in separate columns.\n",
        "\n",
        "  The sentiment will be put in a column called \"{topic}\". The explanation will be put in a column called \"explanation {topic}\".\n",
        "  The confidence score will be put in a column called \"confidence {topic}\"\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  inference_output_for_topic = df[f\"inference_output {topic}\"]\n",
        "  df[topic] = inference_output_for_topic.apply(extract_key_from_json_column(\"sentiment\"))\n",
        "  df[f\"explanation {topic}\"] = inference_output_for_topic.apply(extract_key_from_json_column(\"explanation\"))\n",
        "  df[f\"confidence {topic}\"] = inference_output_for_topic.apply(extract_key_from_json_column(\"confidence\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agzy2dvRb3JL"
      },
      "source": [
        "## Plot inference output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrBVs4c_b3JM"
      },
      "source": [
        "You will use these functions to plot the output after you've run inference on your topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km4hDKjXb3JM"
      },
      "outputs": [],
      "source": [
        "def plot_sentiment_likes_by_topic(df):\n",
        "    \"\"\"Creates a bar plot such that:\n",
        "\n",
        "    - There is one group of bars per topic in SENTIMENT_TOPICS_AND_PROMPTS\n",
        "    - There is a bar for each sentiment type (\"Positive\", \"Negative\", or \"Neutral\")\n",
        "    - The height of each bar is the number of likes on each post.\n",
        "\n",
        "\n",
        "    Example:\n",
        "        # Run inference and extract output\n",
        "        llama_instruct_inference_on_df_for_topic(reddit_df, \"Kamala Harris\")\n",
        "        extract_sentiment_from_inference_output(reddit_df, \"Kamala Harris\")\n",
        "\n",
        "        # Plot sentiment likes by topic\n",
        "        plot_sentiment_likes_by_topic(reddit_df)\n",
        "        plt.savefig(\"reddit_df_kamala_harris_sentiment.pdf\")\n",
        "    \"\"\"\n",
        "    # Ignore not defined warning here: we define SENTIMENT_TOPICS_AND_PROMPTS later\n",
        "    TOPICS_IN_DF = [topic for topic in SENTIMENT_TOPICS_AND_PROMPTS if topic in df]\n",
        "\n",
        "    likes_by_topic_by_sentiment = df.melt(id_vars=\"likes\", value_vars=TOPICS_IN_DF, var_name=\"Topic\", value_name=\"Sentiment\").dropna()\n",
        "\n",
        "    like_sum_by_sentiment_by_topic = likes_by_topic_by_sentiment \\\n",
        "    .groupby([\"Sentiment\", \"Topic\"])[\"likes\"] \\\n",
        "    .sum() \\\n",
        "    .reset_index() \\\n",
        "    .pivot(index=\"Topic\", columns=\"Sentiment\", values=\"likes\") \\\n",
        "\n",
        "    like_sum_by_sentiment_by_topic[like_sum_by_sentiment_by_topic.columns.intersection([\"Negative\", \"Neutral\", \"Positive\"])] \\\n",
        "    .plot(kind=\"bar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ47D9SPb3JM"
      },
      "outputs": [],
      "source": [
        "def plot_sentiment_comment_count_by_topic(df):\n",
        "    \"\"\"Creates a bar plot such that:\n",
        "\n",
        "    - There is one group of bars per topic in SENTIMENT_TOPICS_AND_PROMPTS\n",
        "    - There is a bar for each sentiment type (\"Positive\", \"Negative\", or \"Neutral\")\n",
        "    - The height of each bar is the number of comments on each post.\n",
        "\n",
        "\n",
        "    Example:\n",
        "        # Run inference and extract output\n",
        "        llama_instruct_inference_on_df_for_topic(reddit_df, \"Kamala Harris\")\n",
        "        extract_sentiment_from_inference_output(reddit_df, \"Kamala Harris\")\n",
        "\n",
        "        # Plot sentiment likes by topic\n",
        "        plot_sentiment_comment_count_by_topic(reddit_df)\n",
        "        plt.savefig(\"reddit_df_kamala_harris_sentiment.pdf\")\n",
        "    \"\"\"\n",
        "    # Ignore not defined warning here: we define SENTIMENT_TOPICS_AND_PROMPTS later\n",
        "    TOPICS_IN_DF = [topic for topic in SENTIMENT_TOPICS_AND_PROMPTS if topic in df]\n",
        "\n",
        "    comments_by_topic_by_sentiment = df.melt(value_vars=TOPICS_IN_DF, var_name=\"Topic\", value_name=\"Sentiment\").dropna()\n",
        "\n",
        "    comment_count_by_sentiment_by_topic = comments_by_topic_by_sentiment \\\n",
        "        .groupby([\"Sentiment\", \"Topic\"]) \\\n",
        "        .size() \\\n",
        "        .to_frame(\"Comment count\") \\\n",
        "        .reset_index() \\\n",
        "        .pivot(index=\"Topic\", columns=\"Sentiment\", values=\"Comment count\")\n",
        "\n",
        "    comment_count_by_sentiment_by_topic[comment_count_by_sentiment_by_topic.columns.intersection([\"Negative\", \"Neutral\", \"Positive\"])] \\\n",
        "    .plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UV8ac-Tb3JM"
      },
      "source": [
        "## Sample user prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2uJZXCab3JM"
      },
      "source": [
        "We'll pass the actual text of the posts in as the user prompt, and save the instructions for the system prompt so that the system prompt always stays in context.\n",
        "\n",
        "We've provided you with some sample user prompts to use. Use them, or modify them if you like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e24g31nMb3JM"
      },
      "outputs": [],
      "source": [
        "YOUTUBE_USER_PROMPT_TEMPLATE = \"\"\"\n",
        "Platform: YouTube\n",
        "Channel that posted video: {video_channel}\n",
        "\n",
        "Video title: {video_title}\n",
        "{parent_comment}\n",
        "Comment to analyze:\n",
        "\n",
        "{comment_author}: {comment}\n",
        "\"\"\"\n",
        "\n",
        "def get_youtube_user_prompt(row):\n",
        "  \"\"\"\n",
        "  Formats YOUTUBE_USER_PROMPT_TEMPLATE to work with the given Pandas row.\n",
        "  Row is assumed to be part of a dataframe with the following columns:\n",
        "\n",
        "  body: Text of comment\n",
        "  author: Author of comment\n",
        "  title: Title of video the comment is on\n",
        "  channel: Channel that posted the video that the comment is on\n",
        "  author_parent: Author of the comment that this comment is a reply to\n",
        "  body_parent: Text of the comment that this comment is a reply to\n",
        "\n",
        "  The youtube_df you are provided works with this function.\n",
        "  \"\"\"\n",
        "  if not pd.isna(row.author_parent) and not pd.isna(row.body_parent):\n",
        "    parent_comment = f\"\\nComment is a reply to {row.author_parent}: {row.body_parent}\\n\"\n",
        "  else:\n",
        "    parent_comment = \"\"\n",
        "  return YOUTUBE_USER_PROMPT_TEMPLATE.format(\n",
        "    video_channel=row.channel,\n",
        "    video_title=row.title,\n",
        "    parent_comment=parent_comment,\n",
        "    comment_author=row.author,\n",
        "    comment=row.body\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oReiyBeLb3JM"
      },
      "outputs": [],
      "source": [
        "REDDIT_USER_PROMPT_TEMPLATE = \"\"\"\n",
        "Platform: Reddit\n",
        "Post:\n",
        "{post_author}: {title}\n",
        "{body_post}\n",
        "\n",
        "Comment to analyze:\n",
        "{comment_author}: {comment}\n",
        "\"\"\"\n",
        "\n",
        "def get_reddit_user_prompt(row):\n",
        "    \"\"\"\n",
        "    Formats REDDIT_USER_PROMPT_TEMPLATE to work with the given Pandas row.\n",
        "    Row is assumed to be part of a dataframe with the following columns:\n",
        "\n",
        "    body: Text of comment\n",
        "    author: Author of comment\n",
        "    title: Title of post the comment is on\n",
        "    author_post: Author of the post that this comment is a reply to\n",
        "    body_post: Text of the post that this comment is a reply to\n",
        "\n",
        "    The reddit_df you are provided works with this function.\n",
        "    \"\"\"\n",
        "    return REDDIT_USER_PROMPT_TEMPLATE.format(\n",
        "        post_author=row.author_post,\n",
        "        title=row.title,\n",
        "        body_post=row.body_post,\n",
        "        comment_author=row.author,\n",
        "        comment=row.body\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SEuH8mub3JM"
      },
      "source": [
        "## System prompts (YOUR CODE GOES HERE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcpDUqpEb3JM"
      },
      "source": [
        "The dictionary below contains a list of topics to create prompts for. We have provided you with 4 example prompts that have worked well for us, each following different strategies.\n",
        "\n",
        "These approaches, and some others, are described [on Wikipedia](https://en.wikipedia.org/wiki/Prompt_engineering) in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neZse75Ab3JM"
      },
      "source": [
        "### Breakdown of a few important lines in each prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1KPlB1Zb3JM"
      },
      "source": [
        "\n",
        "> You are given a social media post and a comment to analyze.\n",
        "\n",
        "It is important to provide some context to the model so that it knows what it is looking at.\n",
        "\n",
        "> \"explanation\": \"...\",\n",
        "> \"confidence\": 0-5\n",
        "\n",
        "\n",
        "Telling the model to output a \"confidence\" score and an \"explanation\" actually makes it more likely to make the correct assessment! It also gives us insights on, if the model makes a mistake, why.\n",
        "\n",
        "> Respond only with the JSON. Do not respond with anything else.\n",
        "\n",
        "This line (almost) ensures that the model will output its response in JSON, and exclude any boilerplate (text like \"Here is the JSON object you asked for!\") It isn't perfect, though - the model is unfortunately still prone to errors.\n",
        "\n",
        "You may change the model's output format if you like, but note that you'll need to edit the code that parses the output.\n",
        "\n",
        "This line appears last because it is the most important instruction - if the model outputs sometihng else, then we cannot read its response!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ycLTJRb3JM"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq1CAKKEb3JM"
      },
      "source": [
        "Your task is to adapt the prompts we have provided. First, under the \"Try it!\" section, run a few of our example prompts by changing the TOPIC variable to one of the topics we have provided prompts for. Observe the strengths and weaknesses of each prompt technique.\n",
        "\n",
        "Using your preferred prompting technique, and improving on ours, write prompts that analyze:\n",
        "- Sentiment towards Trump and Harris, in general (TOPIC=\"Donald Trump\" or TOPIC=\"Kamala Harris\")\n",
        "- Sentiment towards Trump and Harris's stances on 2-3 topics of your choosing (can be immigration & the economy like we did, or any other political topic of your choice.)\n",
        "\n",
        "Importantly, note that these prompts sort sentiment into three categories: \"Positive\", \"Negative\", or \"Neutral\". Your code will work with our code as long as the model sorts sentiment into one of these buckets, as well as one extra column for N/A or Not Mentioned (can be named anything, we ignore this.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syl0vfobb3JM"
      },
      "source": [
        "### Prompts (MODIFY THESE!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUkLCVPbb3JM"
      },
      "outputs": [],
      "source": [
        "SENTIMENT_TOPICS_AND_PROMPTS = {\n",
        "    # Basic prompt: Fast, but model is mistake-prone\n",
        "    \"Donald Trump\": \"\"\"\n",
        "        You are given a social media post and a comment to analyze.\n",
        "        Determine the comenter's sentiment toward Donald Trump, if he is mentioned.\n",
        "        Provide the output in the specified dictionary format.\n",
        "\n",
        "        Respond with a JSON object, like:\n",
        "        {\n",
        "            \"sentiment\": \"Positive\", \"Negative\", \"Neutral\", or \"Not Mentioned\",\n",
        "            \"confidence\": 0-5,\n",
        "            \"explanation\": \"...\"\n",
        "        }\n",
        "\n",
        "        Respond only with the JSON. Do not respond with anything else.\n",
        "    \"\"\",\n",
        "\n",
        "    # \"One-shot\" prompt: give the model one example to follow\n",
        "    # Has a nice balance between accuracy and performance,\n",
        "    # but does not give many examples\n",
        "    \"Donald Trump and immigration\": \"\"\"\n",
        "        You are given a social media post and a comment to analyze.\n",
        "        Determine the comenter's sentiment toward Donald Trump's stance on immigration,\n",
        "        if mentioned.\n",
        "\n",
        "        EXAMPLE:\n",
        "        Comment:\n",
        "        sample_user: Vote Trump! Save our border!\n",
        "\n",
        "        Output:\n",
        "        {\n",
        "            \"sentiment\": \"Positive\",\n",
        "            \"confidence\": 3,\n",
        "            \"explanation\": \"The commenter encourages people to vote for Trump, purporting that he will handle the border and thereby immigration well\".\n",
        "        }\n",
        "\n",
        "        Respond with a JSON object, like:\n",
        "        {\n",
        "            \"sentiment\": \"Positive\", \"Negative\", \"Neutral\", or \"Not Mentioned\",\n",
        "            \"confidence\": 0-5,\n",
        "            \"explanation\": \"...\"\n",
        "        }\n",
        "\n",
        "        Respond only with the JSON. Do not respond with anything else.\n",
        "    \"\"\",\n",
        "\n",
        "    # Few-shot: give the model several examples to follow.\n",
        "    # This allows you to tell the model how to handle a number of edge cases.\n",
        "    # This has the highest accuracy, but has a lot of tokens,\n",
        "    # which will have an impact on performance.\n",
        "    \"Donald Trump and the economy\": \"\"\"\n",
        "        You are given a social media post and a comment to analyze.\n",
        "        Determine the comenter's sentiment toward Donald Trump's stance on the economy,\n",
        "        if mentioned.\n",
        "\n",
        "        EXAMPLE:\n",
        "        Comment:\n",
        "        sample_user_1: Joe Biden and Kamala Harris caused this inflation crisis!\n",
        "\n",
        "        Output:\n",
        "        {\n",
        "            \"sentiment\": \"Not Mentioned\",\n",
        "            \"confidence\": 2,\n",
        "            \"explanation\": \"While the comment does mention the economy (inflation) in a negative way, and states that it was caused by Harris, Trump's rival in the 2024 election, it does not directly mention Trump.\"\n",
        "        }\n",
        "\n",
        "        EXAMPLE:\n",
        "        Comment:\n",
        "        sample_user_2: Trump's tariffs will effectively be a $4000/yr sales tax hike. Do you want that for our country?\n",
        "\n",
        "        Output:\n",
        "        {\n",
        "            \"sentiment\": \"Negative\",\n",
        "            \"confidence\": 4,\n",
        "            \"explanation\": \"The comment criticizes Trump's tariffs, implying that it will negatively affect the economy.\"\n",
        "        }\n",
        "\n",
        "        EXAMPLE:\n",
        "        Comment:\n",
        "        sample_user_3: TRUMP 2024!! MAKE AMERICA GREAT AGAIN!!\n",
        "\n",
        "        Output:\n",
        "        {\n",
        "            \"sentiment\": \"Not Mentioned\",\n",
        "            \"confidence\": 5,\n",
        "            \"explanation\": \"The comment does mention Trump, but not the economy.\"\n",
        "        }\n",
        "\n",
        "        EXAMPLE:\n",
        "        Ckmment:\n",
        "        sample_user_4: I'm not sure what Trump's tariffs will do to the economy, but it will be interesting to see it play out!\n",
        "\n",
        "        Output:\n",
        "        {\n",
        "            \"sentiment\": \"Neutral\",\n",
        "            \"confidence\": 5,\n",
        "            \"explanation\": \"The comment talks about Trump's economic policy (tarrifs), but does not make a judgement about them.\"\n",
        "        }\n",
        "\n",
        "        Respond only with the JSON. Do not respond with anything else.\n",
        "    \"\"\",\n",
        "    # Meta Llama's \"knowledge cutoff\" is December 2023.\n",
        "    # That means its training set contains only information from before then.\n",
        "    # Kamala Harris was not a candidate for the 2024 election until July 2024!\n",
        "    # So, adding a small explanation of who Harris is helps a lot.\n",
        "    \"Kamala Harris\": \"\"\"\n",
        "\n",
        "        Kamala Harris is the Democratic party's nominee for President during the 2024 election.\n",
        "        She is currently serving as Vice President of the United States\n",
        "        and won the nomination after Joe Biden dropped out of the race.\n",
        "\n",
        "        You are given a social media post and a comment to analyze.\n",
        "        Determine the comenter's sentiment toward Kamala Harris, if she is mentioned.\n",
        "        Provide the output in the specified dictionary format.\n",
        "\n",
        "        Respond with a JSON object, like:\n",
        "        {\n",
        "            \"sentiment\": \"Positive\", \"Negative\", \"Neutral\", or \"Not Mentioned\",\n",
        "            \"confidence\": 0-5,\n",
        "            \"explanation\": \"...\"\n",
        "        }\n",
        "\n",
        "        Respond only with the JSON. Do not respond with anything else.\n",
        "    \"\"\",\n",
        "    # Chain-of-Thought reasoning: outline a thought process\n",
        "    # for the model to follow. Have the model output it\n",
        "    # before a judgement so that the chain of thought is in its context.\n",
        "    \"Kamala Harris and the economy\": \"\"\"\n",
        "\n",
        "        Kamala Harris is the Democratic party's nominee for President during the 2024 election.\n",
        "        She is currently serving as Vice President of the United States\n",
        "        and won the nomination after Joe Biden dropped out of the race.\n",
        "\n",
        "        You are given a social media post and a comment to analyze.\n",
        "        Determine the comenter's sentiment toward Kamala Harris's stance on the economy,\n",
        "        if she is mentioned. Respond with a JSON object. Reason about this by outlining a logical Chain of thought.\n",
        "\n",
        "        EXAMPLE:\n",
        "        Post:\n",
        "        US consumers see higher long-run inflation, rising delinquency risk\n",
        "\n",
        "        Comment:\n",
        "        sample_user: Do not vote Harris! She caused this\n",
        "\n",
        "        OUTPUT:\n",
        "        {\n",
        "            \"Chain of thought\": \"Comment advises people not to vote for Harris > comment says she caused 'this' > 'this' likely refers to the economic issues referenced in the post > commenter feels Negative towards Kamala Harris'  economy\",\n",
        "            \"sentiment\": \"Negative\",\n",
        "            \"confidence\": 5,\n",
        "            \"explanation\": \"Comment encourages readers to not vote for Harris, claiming she caused the economic problems referenced in the post\".\n",
        "        }\n",
        "\n",
        "        EXAMPLE:\n",
        "        Post:\n",
        "        Kamala Harris up in the polls\n",
        "\n",
        "        Comment:\n",
        "        sample_user: Go queen!\n",
        "\n",
        "        OUTPUT:\n",
        "        {\n",
        "            \"Chain of thought\": \"There is no reference to the economy > Comment does not mention Harris and the economy\",\n",
        "            \"sentiment\": \"N/A\",\n",
        "            \"confidence\": 5,\n",
        "            \"explanation\": \"Although the comment supports Harris, the comment does not support the economy\".\n",
        "        }\n",
        "\n",
        "        Your Chain of thought should end in one of two conclusions:\n",
        "\n",
        "        1. \"commenter feels <Positive|Negative|Neutral> toward Kamala Harris's stance on the economy\"\n",
        "\n",
        "        \"sentiment\" should be the same as what you selected for <Positive|Negative|Neutral> in this case.\n",
        "\n",
        "        2. \"comment does not mention Kamala Harris' stance on the economy\"\n",
        "\n",
        "        \"sentiment\" should be \"N/A\" in this case.\n",
        "\n",
        "        Only analyze the commenter's sentiment towards Kamala Harris' stance on the economy. Do not respond with the sentiment\n",
        "        towards Trump or any other topic. Ensure \"Chain of Thought\" only ends in one of these two conclusions.\n",
        "\n",
        "\n",
        "        Respond with a JSON object, like:\n",
        "        {\n",
        "            \"Chain of thought\": \"...\"\n",
        "            \"sentiment\": \"Positive\", \"Negative\", \"Neutral\", or \"N/A\",\n",
        "            \"confidence\": 0-5,\n",
        "            \"explanation\": \"...\".\n",
        "        }\n",
        "\n",
        "        Respond only with the JSON. Do not respond with anything else.\"\"\",\n",
        "\n",
        "        # YOUR CODE HERE: Analyze a topic of your choice by creating a new key-value pair, like:\n",
        "        \"<YOUR TOPIC HERE>\": \"<YOUR SYSTEM PROMPT HERE>\"\n",
        "}\n",
        "\n",
        "# This code will match up each prompt with its\n",
        "# corresponding entry in each dataframe. Run it each time you\n",
        "# change your prompts.\n",
        "for topic, prompt in SENTIMENT_TOPICS_AND_PROMPTS.items():\n",
        "    youtube_df[f\"system_prompt {topic}\"] = prompt\n",
        "    reddit_df[f\"system_prompt {topic}\"] = prompt\n",
        "youtube_df[\"user_prompt\"] = youtube_df.apply(get_youtube_user_prompt, axis=1)\n",
        "reddit_df[\"user_prompt\"] = reddit_df.apply(get_reddit_user_prompt, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjJTn_7mb3JN"
      },
      "source": [
        "### Try it!\n",
        "\n",
        "Try our prompts, or your own, by runing the following two cells. Read through the model's output to see what it got right and wrong, and use that information to improve your own prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oQph5i5Cb3JN"
      },
      "outputs": [],
      "source": [
        "TOPIC = \"Kamala Harris and the economy\"\n",
        "TRY_DF = reddit_df[:10] # or youtube_df, also try using different slices (e.g. [10:20])\n",
        "\n",
        "llama_instruct_inference_on_df_for_topic(model, tokenizer, TRY_DF, TOPIC)\n",
        "extract_sentiment_from_inference_output(TRY_DF, TOPIC)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "TRY_DF[[\"body\", TOPIC, f\"explanation {TOPIC}\", f\"inference_output {TOPIC}\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIKHrXgZb3JN"
      },
      "source": [
        "You can also plot the output with the following function that we have provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q28Wb2jJb3JN"
      },
      "outputs": [],
      "source": [
        "plot_sentiment_likes_by_topic(TRY_DF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SP_p-dnmxJ7"
      },
      "source": [
        "If inference ever fails due to an OutOfMemoryError:\n",
        "1. Replace the contents of the cell with 1/0 which will delete the exception (Jupyter holds on to a reference to all variables that caused the exception,\n",
        "so memory will not be freed otherwise).\n",
        "2. Run this code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ARd09QmMJI"
      },
      "outputs": [],
      "source": [
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4LvvbjooKet"
      },
      "source": [
        "3. You may have to run the cell that errored out another time to get Jupyter to release the variables associated with the exception."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul7z-5Gqb3JN"
      },
      "source": [
        "## Full Dataset Run: Run Inference on All Topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB49iDMRb3JN"
      },
      "source": [
        "This section takes your prompts for each topic and runs inference on them all.\n",
        "\n",
        "You likely won't run this yourselves. We will run this when running your code on the VM, and send you the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaEU6gAcb3JN"
      },
      "source": [
        "### Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CaVNOmSDb3JN"
      },
      "outputs": [],
      "source": [
        "for topic in SENTIMENT_TOPICS_AND_PROMPTS:\n",
        "    llama_instruct_inference_on_df_for_topic(model, tokenizer, reddit_df, topic)\n",
        "    llama_instruct_inference_on_df_for_topic(model, tokenizer, youtube_df, topic)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ILhz5fb3JN"
      },
      "source": [
        "<!-- #### Save inference output from sentiment back to original dataframes -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Eb0CPBb3JN"
      },
      "source": [
        "### Extract data from inference output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "f1cnN3lWb3JN"
      },
      "outputs": [],
      "source": [
        "for topic in SENTIMENT_TOPICS_AND_PROMPTS:\n",
        "  extract_sentiment_from_inference_output(reddit_df, topic)\n",
        "  extract_sentiment_from_inference_output(youtube_df, topic)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idGMpK5mb3JN"
      },
      "source": [
        "<!-- ### Save inference output from sentiment back to original dataframes -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HBzhF7Ib3JN"
      },
      "source": [
        "## Graph and Save Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Vzl8Ltb3JN"
      },
      "source": [
        "This section produces some visualizations of your data and saves them, along with your After you've run inference, run these cells to save your inference output and create a visualization of your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFTauQFb3JN"
      },
      "source": [
        "### Save raw output to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLcCT6qwb3JO"
      },
      "outputs": [],
      "source": [
        "youtube_df.to_csv(\"youtube.csv\")\n",
        "reddit_df.to_csv(\"reddit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Q9V75nb3JO"
      },
      "source": [
        "### Graph likes by sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tA7itMqb3JO"
      },
      "source": [
        "This code produces graphs that map the like counts of each Reddit and YouTube comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiNfIRD-b3JO"
      },
      "outputs": [],
      "source": [
        "def plot_sentiment_likes_by_topic(df):\n",
        "    \"\"\"\n",
        "    df = join_youtube_dfs(<...>)\n",
        "\n",
        "    llama_instruct_inference_on_df_for_topic(model, df)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    TOPICS_IN_DF = [topic for topic in SENTIMENT_TOPICS_AND_PROMPTS if topic in df]\n",
        "\n",
        "    likes_by_topic_by_sentiment = df.melt(id_vars=\"likes\", value_vars=TOPICS_IN_DF, var_name=\"Topic\", value_name=\"Sentiment\").dropna()\n",
        "\n",
        "    like_sum_by_sentiment_by_topic = likes_by_topic_by_sentiment \\\n",
        "    .groupby([\"Sentiment\", \"Topic\"])[\"likes\"] \\\n",
        "    .sum() \\\n",
        "    .reset_index() \\\n",
        "    .pivot(index=\"Topic\", columns=\"Sentiment\", values=\"likes\") \\\n",
        "\n",
        "    like_sum_by_sentiment_by_topic[like_sum_by_sentiment_by_topic.columns.intersection([\"Negative\", \"Neutral\", \"Positive\"])] \\\n",
        "    .plot(kind=\"bar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pK7n5s4Pb3JO"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_sentiment_likes_by_topic(reddit_df)\n",
        "plt.suptitle(\"Upvotes on Reddit comments, grouped by sentiment towards topics\")\n",
        "plt.ylabel(\"Total Upvotes\")\n",
        "plt.savefig(\"reddit_upvotes_grouped_by_sentiment.png\", bbox_inches=\"tight\")\n",
        "\n",
        "plot_sentiment_likes_by_topic(youtube_df)\n",
        "plt.suptitle(\"Likes on YouTube comments, grouped by sentiment towards topics\")\n",
        "plt.ylabel(\"Total Likes\")\n",
        "plt.savefig(\"youtube_likes_grouped_by_sentiment.png\", bbox_inches=\"tight\")\n",
        "\n",
        "plot_sentiment_comment_count_by_topic(reddit_df)\n",
        "plt.suptitle(\"Comment count of Reddit comments, grouped by sentiment towards topics\")\n",
        "plt.ylabel(\"Total number of comments\")\n",
        "plt.savefig(\"reddit_comments_grouped_by_sentiment.png\", bbox_inches=\"tight\")\n",
        "\n",
        "plot_sentiment_comment_count_by_topic(youtube_df)\n",
        "plt.suptitle(\"Comment count of YouTube comments, grouped by sentiment towards topics\")\n",
        "plt.ylabel(\"Total number of comments\")\n",
        "plt.savefig(\"youtube_comments_grouped_by_sentiment.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9rNWCwyb3JO"
      },
      "source": [
        "## Report (YOUR WORK HERE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oOfT36Ob3JO"
      },
      "source": [
        "Provide a brief answer (ideally < 1 paragraph each, up to 2) to the following questions:\n",
        "\n",
        "1. Explain your method for data collection (What search terms did you use? What subreddits did you analyze?) Why did you choose this method?\n",
        "2. Pick 1 or more of the prompting techniques you used. How well did it work? Can you use your knowledge of how LLMs work to explain why?\n",
        "3. As you ran your code each day, how did sentiment about your candidate toward your selected issue(s) change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ6Ak39Wb3JO"
      },
      "source": [
        "## Fine-tune (EXTRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFm_RhEb3JO"
      },
      "source": [
        "You've likely noticed by this point that, with prompt engineering, there is an upper limit to how well we can do.\n",
        "\n",
        "Few-shot and Chain of Thought show better results, but they come at the cost of increasing the number of tokens in the prompt. Sometimes, this means the model can miss small details! And, because they take up more tokens, our batch size has to come down in order to fit on a GPU, meaning we lose on parallelism.\n",
        "\n",
        "Fine-tuning is a process by which the weights of an LLM are slightly tweaked in order to modify the output. With modern techniques, the results can be shocking.\n",
        "\n",
        "Follow the instructions in fine_tune.ipynb to fine-tune your model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z75ny6_mb3JD",
        "THI-C1-Ab3JE",
        "ieg1TFjab3JF",
        "_T-vA_xlb3JF",
        "CgBjyN-Ub3JF",
        "7DJeRu5Nb3JF",
        "41JI4x2Tb3JF",
        "G7AQGagKb3JG",
        "WyfojXzmb3JL",
        "Agzy2dvRb3JL",
        "syl0vfobb3JM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
